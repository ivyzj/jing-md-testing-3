{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxDome Notebooks\n",
    "\n",
    "MaxDome notebooks are Serverless PySpark notebooks powered by AWS Glue Interactive Sessions.\n",
    "To Create a session and begin using PySpark simply run a cell of code. \n",
    "\n",
    "## Additional Sample Notebooks\n",
    "\n",
    "Additonal sample notebooks are available on the aws-glue-sample repo in GitHub and can be imported to your project using the upload button on the top of the file browser in Jupyter.\n",
    "\n",
    "https://github.com/aws-samples/aws-glue-samples/tree/master/examples/notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Configuration\n",
    "\n",
    "MaxDome notebooks are configured via Jupyter magics (commands prefixed with `%` and `%%`). In MaxDome and Glue IS, these are used to configure the PySpark environment including cluster size, shape and installed libraries. \n",
    "\n",
    "run `%help` in an empty cell to see the full list of magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to configure your PySpark environment. \n",
    "%additional_python_modules sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Start Session and configure Spark\n",
    "import sys\n",
    "import boto3\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "args = getResolvedOptions(\n",
    "    sys.argv, [\"redshift_url\", \"redshift_iam_role\", \"redshift_tempdir\"]\n",
    ")\n",
    "\n",
    "# These exist in Sessions automatically. Adding for Linter\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get the MaxDome Database info\n",
    "glue_client = boto3.client(\"glue\")\n",
    "\n",
    "# filter Glue Databases for databases that start with \"maxdome_producer_db\"\n",
    "databases_paginator = glue_client.get_paginator(\"get_databases\")\n",
    "response_iterator = databases_paginator.paginate()\n",
    "glue_databases = response_iterator.build_full_result().get(\"DatabaseList\")\n",
    "maxdome_database = [\n",
    "    db for db in glue_databases if db[\"Name\"].startswith(\"maxdome_producer_db\")\n",
    "][0]\n",
    "\n",
    "maxdome_database_name = maxdome_database[\"Name\"]\n",
    "maxdome_database_location = maxdome_database[\"LocationUri\"]\n",
    "print(f\"Project Database Name: {maxdome_database_name}\")\n",
    "print(f\"Project Database Location: {maxdome_database_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame from NYC Taxi Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2009-01.parquet\"\n",
    ").limit(1000)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the column names to match Catalog's casing specifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "renamed_df = df.select(\n",
    "    col(\"vendor_name\").alias(\"vendor_id\"),\n",
    "    col(\"Trip_Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "    col(\"Trip_Dropoff_DateTime\").alias(\"dropoff_datetime\"),\n",
    "    col(\"Passenger_Count\").alias(\"passenger_count\"),\n",
    "    col(\"Trip_Distance\").alias(\"trip_distance\"),\n",
    "    col(\"Start_Lon\").alias(\"pickup_longitude\"),\n",
    "    col(\"Start_Lat\").alias(\"pickup_latitude\"),\n",
    "    col(\"Rate_Code\").alias(\"rate_code\"),\n",
    "    col(\"store_and_forward\").alias(\"store_and_fwd_flag\"),\n",
    "    col(\"End_Lon\").alias(\"dropoff_longitude\"),\n",
    "    col(\"End_Lat\").alias(\"dropoff_latitude\"),\n",
    "    col(\"Payment_Type\").alias(\"payment_type\"),\n",
    "    col(\"Fare_Amt\").alias(\"fare_amount\"),\n",
    "    col(\"surcharge\").alias(\"surcharge\"),\n",
    "    col(\"mta_tax\").alias(\"mta_tax\"),\n",
    "    col(\"Tip_Amt\").alias(\"tip_amount\"),\n",
    "    col(\"Tolls_Amt\").alias(\"tolls_amount\"),\n",
    "    col(\"Total_Amt\").alias(\"total_amount\"),\n",
    ")\n",
    "renamed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the dataframe to a catalog table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to the data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "DATABASE = maxdome_database_name\n",
    "TABLE = \"nyc_yellow\"\n",
    "S3_PATH = f\"{maxdome_database_location}{TABLE}/\"\n",
    "(\n",
    "    renamed_df.write.mode(\"append\")\n",
    "    .option(\"path\", S3_PATH)\n",
    "    .saveAsTable(f\"{DATABASE}.{TABLE}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data back from Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.sql(f\"select * from {DATABASE}.{TABLE}\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Operations\n",
    "\n",
    "Redshift connectivity is currently handled by the community Spark Connector for Redshift and is best suited for moving large amounts of data between Redshift and Spark for use with PySpark or pandas.\n",
    "> A library to load data into Spark SQL DataFrames from Amazon Redshift, and write them back to Redshift tables. Amazon S3 is used to efficiently transfer data in and out of Redshift, and JDBC is used to automatically trigger the appropriate COPY and UNLOAD commands on Redshift.\n",
    ">\n",
    "> This library is more suited to ETL than interactive queries, since large amounts of data could be extracted to S3 for each query execution. If you plan to perform many queries against the same Redshift tables then we recommend saving the extracted data in a format such as Parquet.\n",
    "\n",
    "Parameters and documentation can be found [on GitHub](https://github.com/spark-redshift-community/spark-redshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to Redshift with IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rs_database = \"dev\"\n",
    "rs_table = \"project.nyc_taxi_yellow\"\n",
    "\n",
    "(\n",
    "    renamed_df.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", args[\"redshift_url\"])\n",
    "    .option(\"dbtable\", rs_table)\n",
    "    .option(\"tempdir\", args[\"redshift_tempdir\"])\n",
    "    .option(\"aws_iam_role\", args[\"redshift_iam_role\"])\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rs_read_df = (\n",
    "    spark.read.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", args[\"redshift_url\"])\n",
    "    .option(\"aws_iam_role\", args[\"redshift_iam_role\"])\n",
    "    .option(\"tempdir\", args[\"redshift_tempdir\"])\n",
    "    .option(\"unload_s3_format\", \"PARQUET\")\n",
    "    .option(\"dbtable\", rs_table)\n",
    "    .load()\n",
    ")\n",
    "rs_read_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pandas On Spark\n",
    "\n",
    "For those new to or unfamiliar with PySpark the Pandas API on Spark may allow for a more familiar interface. Using the library much of the operations on a Spark DataFrame can be executed using familiar pandas APIs while taking advantage of many of Spark's optimizations and distributed compute. \n",
    "\n",
    "See the [Spark documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html) for a deeper dive\n",
    "\n",
    "For a full example see the `nyc_taxi_predictions` sample notebook next to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = ps.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2009-01.parquet\")\n",
    "psdf.info()\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sagemaker\n",
    "\n",
    "You can utilized The Sagemaker PySDK from your PySpark notebook by installing it with `%additional_python_modules` at the start of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
